---
title: "Jeffreys' Bayes Factor Approximation with Wald statistic"
output: html_notebook
---

EJ: "The factor inside the exponent (other than the -0.5 bit) is the Wald statistic W. This is distributed as a $\chi^2$ with 1 df. So I take the p-value from the t-test, put it through the quantile function of the $\chi^2$ test to obtain the matching W, and plug it into Jeffreys equation."

```{r}
# Approximate BF10 is 1/K
BFapproxJW <- function(tval, n, alternative){
  
  # Reconstruct p-value of t-test function for t value input
  lowerTail <- ifelse(alternative=="less", T, F)
  df <- n-1
  pval <- ifelse(alternative=="two.sided",
                 min(pt(q=tval, df=df, lower.tail=F),
                     pt(q=tval, df=df, lower.tail=T))*2,
                 pt(q=tval, df=df, lower.tail=lowerTail))
  
  # Compute Wald statistic
  W <- qchisq(pval, df=1, lower.tail=FALSE)
  
  # Jeffreys approximation with Wald statistic
  1/(sqrt(n)*exp(-0.5*W))
}

# Compare to Bayes factor (one value)
library(BayesFactor)
ttest.tstat(t = 1.5, n1 = 50, simple=T)
BFapproxJW(tval=1.5, n = 50, alternative="two.sided")
```

### Check approximation for multiple values of t and multiple sample sizes

```{r}
tval <- seq(-5, 5, length.out=1001)
Ns <- c(10, 50, 100, 500)
cond <- expand.grid(tval, Ns)

cond$BFapproxJW <- apply(cond[, 1:2], 1, FUN = function(x) BFapproxJW(t=x[1], n=x[2], alternative = "two.sided"))
cond$BFttest <- apply(cond[, 1:2], 1, FUN = function(x) unname(ttest.tstat(t=x[1], n1=x[2], simple=T)))

colnames(cond)[1:2] <- c("tval", "N")

```


```{r}
library(dplyr)
library(ggplot2)

cond %>% 
  ggplot(aes(tval, log(BFapproxJW))) + 
  geom_line() + 
  facet_wrap(~ N) +
  geom_line(aes(tval, log(BFttest)), color = "seagreen") +
  labs(y = "log BF", x = "t-value")

```

### Directional Tests

__Alternative: Greater__

```{r}
tval <- seq(-5, 5, length.out=1001)
Ns <- c(10, 50, 100, 500)
cond <- expand.grid(tval, Ns)

cond$BFapproxJW <- apply(cond[, 1:2], 1, FUN = function(x) BFapproxJW(t=x[1], n=x[2], alternative = "greater"))
cond$BFttest <- apply(cond[, 1:2], 1, FUN = function(x) unname(ttest.tstat(t=x[1], n1=x[2], simple=T, nullInterval = c(0, Inf))))

colnames(cond)[1:2] <- c("tval", "N")

```


```{r}
library(dplyr)
library(ggplot2)

cond %>% 
  ggplot(aes(tval, log(BFapproxJW))) + 
  geom_line() + 
  facet_wrap(~ N) +
  geom_line(aes(tval, log(BFttest)), color = "seagreen") +
  labs(y = "log BF", x = "t-value")

```

__Alternative: Less__

```{r}
tval <- seq(-5, 5, length.out=1001)
Ns <- c(10, 50, 100, 500)
cond <- expand.grid(tval, Ns)

cond$BFapproxJW <- apply(cond[, 1:2], 1, FUN = function(x) BFapproxJW(t=x[1], n=x[2], alternative = "less"))
cond$BFttest <- apply(cond[, 1:2], 1, FUN = function(x) unname(ttest.tstat(t=x[1], n1=x[2], simple=T, nullInterval = c(-Inf, 0))))

colnames(cond)[1:2] <- c("tval", "N")

```


```{r}
library(dplyr)
library(ggplot2)

cond %>% 
  ggplot(aes(tval, log(BFapproxJW))) + 
  geom_line() + 
  facet_wrap(~ N) +
  geom_line(aes(tval, log(BFttest)), color = "seagreen") +
  labs(y = "log BF", x = "t-value")

```


__Observations:__

- Jeffreys' approximation is much better using the Wald statistic than using the t-value
- The approximation is still better for larger N than for smaller N, and for small t-values than for large t-values
- Also still won't work with non-default prior distributions

### Using the Jeffrey+Wald Approximation for BFDA

```{r}
Ns <- c(10, 50, 100, 500)
deltas <- c(0, 0.1, 0.3, 0.5, 0.8)
priorwidth <- c(0.707, 1)

conds <- expand.grid(Ns, deltas)
tvals <- simplify2array(apply(conds, 1, function(x){
  tval <- NA
  for(i in 1:5000){
    samp <- rnorm(x[1], x[2], sd=1)
    tval[i] <- t.test(samp)$statistic
  }
  return(tval)
  }))

res <- array(NA, dim=c(5000, (length(Ns)*length(deltas)), 3))

for(i in 1:(length(Ns)*length(deltas))){
  
  res[, i, 1] <- log(apply(cbind(tvals[,i]), 1, function(x) unname(ttest.tstat(x, n1 = conds[i,1], rscale = "medium", simple=T))))
  res[, i, 2] <- log(apply(cbind(tvals[,i]), 1, function(x) unname(ttest.tstat(x, n1 = conds[i,1], rscale = "wide", simple=T))))
  res[, i, 3] <- log(apply(cbind(tvals[,i]), 1, function(x) BFapproxJW(x, n = conds[i,1], alternative="two.sided")))
  
}

```


```{r}
bxpdat <- list()
par(mfrow=c(1,4), mar=c(5, 3, 4, 1))

for(i in 1:(length(Ns)*length(deltas))){
  bxpdat[[i]] <- as.data.frame(matrix(NA, nrow=3*5000, ncol=2, dimnames = list(NULL, c("method", "BF"))))
  bxpdat[[i]]$method <- rep(1:3, each=5000)
  for(j in 1:3){
    bxpdat[[i]]$BF[ bxpdat[[i]]$method==j] <- res[, i, j]
  }
  boxplot(BF ~ method, bxpdat[[i]], main=paste0("N = ", conds[i, 1], ", ES = ", conds[i, 2]), col=c("seagreen", "seagreen1", "grey"))
  
}
```


__Observations:__

- Jeffreys approximation using the Wald statistic is a better approximation for the default Bayes factor (dark green) in the t-test than for a Bayes factor that uses a wide prior (r = 1, light green)
- It works better for large sample sizes (N > 100) and large effect sizes (delta > 0.5).
- Maybe a rule of thumb: In areas where the prior matters (e.g., small N), Jeffreys approximation is not doing very well.


